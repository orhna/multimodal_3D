{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef899b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from os.path import join, exists\n",
    "import open3d as o3d\n",
    "import clip\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0088f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 3D scene path (should be pre-processed)\n",
    "raw_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/scannet_3d/example/scene0000_00_vh_clean_2.pth\"\n",
    "# need to have distilled features ready\n",
    "distilled_feature_path = \"/mnt/project/AT3DCV_Data/3D_features/scene0000_00_vh_clean_2_openscene_feat_distill.npy\"\n",
    "# fused features\n",
    "fused_feature_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/test_whole/scene0000_00_0.pt\"\n",
    "\n",
    "# for augmented\n",
    "# sample 3D scene path (should be pre-processed)\n",
    "raw_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/scannet_3d/example/scene0000_00_vh_clean_2.pth\"\n",
    "# need to have distilled features ready\n",
    "distilled_feature_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/features_3D/scene0000_00_vh_clean_2_openscene_feat_distill.npy\"\n",
    "# fused features\n",
    "fused_feature_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/fused/scene0000_00_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542308ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading original sample\n",
    "raw_sample = torch.load(raw_path) \n",
    "raw_sample_points = raw_sample[0]\n",
    "raw_sample_colors = raw_sample[1]\n",
    "\n",
    "# loading fused 2D features\n",
    "fused_f_d = torch.load(fused_feature_path)\n",
    "\n",
    "fused_f = fused_f_d[\"feat\"]\n",
    "# Get the indices where the mask is True\n",
    "inds_reverse = torch.nonzero(fused_f_d[\"mask_full\"]).squeeze()\n",
    "\n",
    "# loading distilled 3D features\n",
    "distilled = np.load(distilled_feature_path)\n",
    "#cast and normalize embeddings for distilled \n",
    "distilled_f = torch.from_numpy(distilled).half()\n",
    "# masking to match the features with 2D fused ones\n",
    "distilled_f = distilled_f[inds_reverse, :]\n",
    "distilled_f = distilled_f / distilled_f.norm(p=2, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6ef5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# get text embeddings with clip\n",
    "# type the query here \n",
    "query = [\"table\"]\n",
    "with torch.no_grad():\n",
    "    all_text_embeddings = []\n",
    "    for category in tqdm(query):\n",
    "        texts = clip.tokenize(category)  #tokenize\n",
    "        texts = texts.cuda()\n",
    "        text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embeddings.mean(dim=0)\n",
    "        text_embedding /= text_embedding.norm()\n",
    "        all_text_embeddings.append(text_embedding)\n",
    "\n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30076c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_fusion = fused_f.half().cuda() @ all_text_embeddings\n",
    "pred_fusion = (fused_f/(fused_f.norm(dim=-1, keepdim=True)+1e-5)).half().cuda() @ all_text_embeddings\n",
    "\n",
    "pred_distill = (distilled_f/(distilled_f.norm(dim=-1, keepdim=True)+1e-5)).half().cuda() @ all_text_embeddings\n",
    "\n",
    "feat_ensemble = distilled_f.clone().half()\n",
    "\n",
    "mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "\n",
    "feat_ensemble[mask_] = fused_f[mask_]\n",
    "\n",
    "# after masking, need to normalize feat_ensemble here because we're picking the points we'll highlight\n",
    "# by a certain threshold so it has to be normalized to compare other approaches with only 2D or 3D.\n",
    "# if we do evaluating with max logits and labels, then there is no need for a normalization here\n",
    "similarity_matrix = (feat_ensemble/(feat_ensemble.norm(dim=-1, keepdim=True)+1e-5)).cuda() @ all_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set higher to increase the certainty (not always correct)\n",
    "threshold_percentage = 0.9\n",
    "cap = similarity_matrix.max().item()\n",
    "found_indices = torch.nonzero(similarity_matrix > cap*threshold_percentage, as_tuple=False).squeeze().T[0]\n",
    "\n",
    "# creating pc\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(raw_sample_points[inds_reverse,:]))\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(raw_sample_colors[inds_reverse,:]))\n",
    "\n",
    "found_region = pcd.select_by_index(found_indices.tolist())\n",
    "found_region.paint_uniform_color([1.0, 0, 0]) # paint related points to red\n",
    "rest = pcd.select_by_index(found_indices.tolist(), invert=True)\n",
    "o3d.visualization.draw_geometries([rest,found_region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7f2a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1551])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef1ce7",
   "metadata": {},
   "source": [
    "# experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f731e01",
   "metadata": {},
   "source": [
    "here we create a zero matrix from the distilled features because it has a feature vector for all of the original points from the raw sample, then we're filling the zeros according to the mask of the corresponding points between 3D-2D with the 2D fused features to get the extended fused features tensor. There will still be zeros in this extended fused feature tensor because of the missing points from 2D data. However, our base feature tensor is from 3D distilled features, and zeros would be replaced by those. In this way we're only considering the 3D distilled features for the missing points because of the 2D data and we're able to visualize all the original points from 3D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba2eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading original sample\n",
    "raw_sample = torch.load(raw_path) \n",
    "raw_sample_points = raw_sample[0]\n",
    "raw_sample_colors = raw_sample[1]\n",
    "\n",
    "# loading fused 2D features\n",
    "fused_f_d = torch.load(fused_feature_path)\n",
    "fused_f = fused_f_d[\"feat\"]\n",
    "# Get the indices where the mask is True\n",
    "inds_reverse = torch.nonzero(fused_f_d[\"mask_full\"]).squeeze()\n",
    "\n",
    "# loading distilled 3D features\n",
    "distilled = np.load(distilled_feature_path)\n",
    "#cast and normalize embeddings for distilled \n",
    "distilled_f = torch.from_numpy(distilled).half()\n",
    "fused_extended = torch.zeros_like(distilled_f)\n",
    "distilled_f = distilled_f / distilled_f.norm(p=2, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471d4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_extended[inds_reverse,:] = fused_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470de6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fusion = (fused_extended/(fused_extended.norm(dim=-1, keepdim=True)+1e-5)).half().cuda() @ all_text_embeddings\n",
    "\n",
    "pred_distill = (distilled_f/(distilled_f.norm(dim=-1, keepdim=True)+1e-5)).half().cuda() @ all_text_embeddings\n",
    "\n",
    "feat_ensemble = distilled_f.clone().half()\n",
    "\n",
    "mask_ =  pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "\n",
    "feat_ensemble[mask_] = fused_extended[mask_]\n",
    "\n",
    "similarity_matrix = (feat_ensemble/(feat_ensemble.norm(dim=-1, keepdim=True)+1e-5)).cuda() @ all_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8c2178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([81369, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8992594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set higher to increase the certainty (not always correct)\n",
    "threshold_percentage = 0.4\n",
    "cap = similarity_matrix.max().item()\n",
    "found_indices = torch.nonzero(similarity_matrix > cap*threshold_percentage, as_tuple=False).squeeze().T[0]\n",
    "\n",
    "# creating pc\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(raw_sample_points))\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(raw_sample_colors))\n",
    "\n",
    "found_region = pcd.select_by_index(found_indices.tolist())\n",
    "found_region.paint_uniform_color([1.0, 0, 0]) # paint related points to red\n",
    "rest = pcd.select_by_index(found_indices.tolist(), invert=True)\n",
    "o3d.visualization.draw_geometries([rest,found_region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d4aa22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5254])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_indices.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
