{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37317be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from os.path import join, exists\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34fb0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scene(path, visualize = True):\n",
    "    \n",
    "    sample = torch.load(path)\n",
    "    sample_points  = sample[0]\n",
    "    sample_colors = sample[1]\n",
    "    sample_labels = sample[2]\n",
    "    \n",
    "    if visualize:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(np.asarray(sample_points))\n",
    "        pcd.colors = o3d.utility.Vector3dVector(np.asarray(sample_colors))\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "        \n",
    "    return sample_points, sample_colors, sample_labels\n",
    "\n",
    "def load_fused_features(path, sample_points, sample_colors, sample_labels):\n",
    "    \n",
    "    features = torch.load(path)\n",
    "    indices = torch.nonzero(features[\"mask_full\"]).squeeze()\n",
    "    filtered_point_cloud = sample_points[indices, :]\n",
    "    filtered_point_cloud_colors = sample_colors[indices, :]\n",
    "    filtered_point_cloud_labels = sample_labels[indices]\n",
    "    fused_features = (features[\"feat\"]/(features[\"feat\"].norm(dim=-1, keepdim=True)+1e-5))\n",
    "    \n",
    "    return fused_features, filtered_point_cloud, filtered_point_cloud_colors, filtered_point_cloud_labels, indices\n",
    "\n",
    "def load_distilled_features(path, indices):\n",
    "    \n",
    "    distilled = np.load(path)\n",
    "    #cast and normalize embeddings for distilled \n",
    "    distilled = distilled[indices, :]\n",
    "    distilled_t = torch.from_numpy(distilled).half()\n",
    "    distilled_f = (distilled_t/(distilled_t.norm(dim=-1, keepdim=True)+1e-5))\n",
    "    \n",
    "    return distilled_f\n",
    "\n",
    "def highlight_query(query, feature_type, agg_type, distill, fused, fpc, fpcc, device):\n",
    "    \n",
    "    import clip\n",
    "    model, preprocess = clip.load(\"ViT-L/14@336px\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        per_descriptor_embeds = []\n",
    "        for descriptor in tqdm(query):\n",
    "            texts = clip.tokenize(descriptor)  #tokenize\n",
    "            texts = texts.to(device)\n",
    "            text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            per_descriptor_embeds.append(text_embeddings)\n",
    "\n",
    "        per_descriptor_embeds = torch.stack(per_descriptor_embeds, dim=1).squeeze()\n",
    "\n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ per_descriptor_embeds.T\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ per_descriptor_embeds.T\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ per_descriptor_embeds\n",
    "        pred_distill = distill.to(device) @ per_descriptor_embeds.T\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble @ per_descriptor_embeds.T\n",
    "        \n",
    "    if agg_type == \"mean\":\n",
    "        agg_sim_mat = torch.mean(similarity_matrix, dim=1)\n",
    "    elif agg_type == \"max\":\n",
    "        agg_sim_mat, _ = torch.max(similarity_matrix, dim=1)\n",
    "        \n",
    "    agg_sim_mat = agg_sim_mat.reshape(-1, 1)\n",
    "    \n",
    "    # creating pc\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.asarray(fpc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.asarray(fpcc))\n",
    "\n",
    "    # heatmap\n",
    "    cmap = plt.get_cmap('cividis')\n",
    "\n",
    "    # normalize the tensor to the range [0, 1]\n",
    "    normalized_tensor = (agg_sim_mat - torch.min(agg_sim_mat)) / (torch.max(agg_sim_mat) - torch.min(agg_sim_mat))\n",
    "\n",
    "    colors = cmap(normalized_tensor.detach().cpu().numpy().squeeze())\n",
    "    pcd_heatmap = o3d.geometry.PointCloud()\n",
    "\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(pcd.points)\n",
    "    pcd_heatmap.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "\n",
    "    #transform heatmap to the side\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) + [0,10,0])\n",
    "\n",
    "    o3d.visualization.draw_geometries([pcd, pcd_heatmap])\n",
    "    \n",
    "    return agg_sim_mat\n",
    "\n",
    "def confusion_matrix(pred_ids, gt_ids, num_classes):\n",
    "    '''calculate the confusion matrix.'''\n",
    "\n",
    "    assert pred_ids.shape == gt_ids.shape, (pred_ids.shape, gt_ids.shape)\n",
    "    idxs = gt_ids != UNKNOWN_ID\n",
    "    if NO_FEATURE_ID in pred_ids: # some points have no feature assigned for prediction\n",
    "        pred_ids[pred_ids==NO_FEATURE_ID] = num_classes\n",
    "        confusion = np.bincount(\n",
    "            pred_ids[idxs] * (num_classes+1) + gt_ids[idxs],\n",
    "            minlength=(num_classes+1)**2).reshape((\n",
    "            num_classes+1, num_classes+1)).astype(np.ulonglong)\n",
    "        return confusion[:num_classes, :num_classes]\n",
    "\n",
    "    return np.bincount(\n",
    "        pred_ids[idxs] * num_classes + gt_ids[idxs],\n",
    "        minlength=num_classes**2).reshape((\n",
    "        num_classes, num_classes)).astype(np.ulonglong)\n",
    "\n",
    "def get_iou(label_id, confusion):\n",
    "    '''calculate IoU.'''\n",
    "\n",
    "    # true positives\n",
    "    tp = np.longlong(confusion[label_id, label_id])\n",
    "    # false positives\n",
    "    fp = np.longlong(confusion[label_id, :].sum()) - tp\n",
    "    # false negatives\n",
    "    fn = np.longlong(confusion[:, label_id].sum()) - tp\n",
    "\n",
    "    denom = (tp + fp + fn)\n",
    "    if denom == 0:\n",
    "        return float('nan')\n",
    "    return float(tp) / denom, tp, denom\n",
    "\n",
    "def evaluate(labelset, descriptors, feature_type, agg_type, distill, fused, gt_ids):\n",
    "    \n",
    "    import clip\n",
    "    model, preprocess = clip.load(\"ViT-L/14@336px\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        label_embeds = []\n",
    "        for category in tqdm(labelset):\n",
    "            texts = clip.tokenize(category)  #tokenize\n",
    "            texts = texts.cuda()\n",
    "            text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            label_embeds.append(text_embeddings)\n",
    "\n",
    "    label_embeds = torch.cat(label_embeds, dim=0) # has the shape of [768, *original labels*+*descriptors*]\n",
    "    \n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ label_embeds.T\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ label_embeds.T\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ label_embeds.T\n",
    "        pred_distill = distill.to(device) @ label_embeds.T\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble.to(device) @ label_embeds.T\n",
    "    \n",
    "    # separating the similarity matrix for the original labels and descriptors\n",
    "    sim_labels = similarity_matrix[:, :20]\n",
    "    sim_descriptors = similarity_matrix[:, 20:]\n",
    "    \n",
    "    # get lengths of the descriptors\n",
    "    descriptor_lengths = []\n",
    "    for label in labelset:\n",
    "        if not isinstance(label, str):\n",
    "            descriptor_lengths.append(len(label))\n",
    "            \n",
    "    # aggregate the corresponding descriptor vectors by the length of each\n",
    "    # keep them in a list to stack it later\n",
    "    _idx = 0\n",
    "    agg_desc= []\n",
    "    for elem in descriptor_lengths:\n",
    "        if agg_type == \"mean\":\n",
    "            agg_desc_sim_mat = torch.mean(sim_descriptors[:, _idx : _idx + elem], dim=1)\n",
    "        elif agg_type == \"max\":\n",
    "            agg_desc_sim_mat, _ = torch.max(sim_descriptors[:, _idx : _idx + elem], dim=1)\n",
    "        \n",
    "        _idx += elem\n",
    "        agg_desc.append(agg_desc_sim_mat)\n",
    "        \n",
    "    # stack aggregated descriptor similarity matrices\n",
    "    agg_sim_descriptors = torch.stack(agg_desc, dim = 1)\n",
    "    # combine with the similarity matrix of labels\n",
    "    agg_sim_mat = torch.cat([sim_labels,agg_sim_descriptors], dim = 1)\n",
    "\n",
    "    # get the predictions\n",
    "    pred_ids = torch.max(agg_sim_mat, 1)[1].detach().cpu()    \n",
    "    \n",
    "    N_CLASSES = len(labelset)\n",
    "    confusion = confusion_matrix(pred_ids, gt_ids, N_CLASSES)\n",
    "    class_ious = {}\n",
    "    class_accs = {}\n",
    "    mean_iou = 0\n",
    "    mean_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(N_CLASSES):\n",
    "        label_name = labelset[i]\n",
    "\n",
    "        if not isinstance(label_name, str): \n",
    "            for key, value in descriptors.items():\n",
    "                if value == label_name:\n",
    "                    label_name = key\n",
    "                    \n",
    "        if (gt_ids==i).sum() == 0: # at least 1 point needs to be in the evaluation for this class\n",
    "            continue\n",
    "\n",
    "        class_ious[label_name] = get_iou(i, confusion)\n",
    "        class_accs[label_name] = class_ious[label_name][1] / (gt_ids==i).sum()\n",
    "        count+=1\n",
    "\n",
    "        mean_iou += class_ious[label_name][0]\n",
    "        mean_acc += class_accs[label_name]\n",
    "\n",
    "    mean_iou /= N_CLASSES\n",
    "    mean_acc /= N_CLASSES\n",
    "    \n",
    "    return class_ious, class_accs, mean_iou, mean_acc\n",
    "\n",
    "def print_results(labelset, class_ious):\n",
    "    \n",
    "    print('classes                 IoU')\n",
    "    print('----------------------------')\n",
    "    for i in range(len(labelset)):\n",
    "        label_name = labelset[i]\n",
    "        if not isinstance(label_name, str): \n",
    "            for key, value in descriptors.items():\n",
    "                if value == label_name:\n",
    "                    label_name = key\n",
    "        try:\n",
    "            print('{0:<14s}             :          {1:>5.5f}           ({2:>6d}/{3:<6d})'.format(\n",
    "                    label_name,\n",
    "                    class_ious[label_name][0],\n",
    "                    class_ious[label_name][1],\n",
    "                    class_ious[label_name][2]))\n",
    "        except:\n",
    "            print(label_name + ' error!')\n",
    "            continue\n",
    "            \n",
    "def descriptors_from_prompt(text, verbose = True):\n",
    "    \n",
    "    import openai\n",
    "\n",
    "    openai.api_key = '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'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=text,\n",
    "\n",
    "      temperature=0.5,\n",
    "      max_tokens=200\n",
    "    )\n",
    "    \n",
    "    print(response[\"choices\"][0].text)\n",
    "    \n",
    "    lines = [s for s in [line.strip() for line in response[\"choices\"][0].text.splitlines()] if s]\n",
    "    \n",
    "    descriptors = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.split(\":\")\n",
    "        key = parts[0].strip()\n",
    "        features = [f'a bird which is {f.strip()}.' for f in parts[1].split(\",\")]\n",
    "        descriptors[key] = features\n",
    "        \n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbaa988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/birds/scannet_3d/example/scene0000_00_vh_clean_2.pth\"\n",
    "fused_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/birds/fused/scene0000_00_0.pt\"\n",
    "distilled_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/birds/features_3D/scene0000_00_vh_clean_2_openscene_feat_distill.npy\"\n",
    "\n",
    "source_points, source_colors, source_labels = load_scene(source_path, False)\n",
    "\n",
    "fused_f, filtered_pc, filtered_pc_c, filtered_pc_labels, indices = load_fused_features(fused_path,\n",
    "                                                                              source_points, \n",
    "                                                                              source_colors,\n",
    "                                                                              source_labels)\n",
    "distilled_f = load_distilled_features(distilled_path, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff506b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about scene with birds\n",
    "# diamond firetail         class label = 20, 1798 points, on the bed\n",
    "# Blue-faced Honeyeater    class label = 21, 1771 points, on the kitchen counter\n",
    "# Mouse-colored Tyrannulet class label = 22, 1476 points, on the sofa corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b12ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 98.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n",
      "torch.Size([82723, 768])\n",
      "torch.Size([82723, 3])\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "query = [\"a bird which is grey-breasted\", \"a bird which is brown-crowned\", \"a bird which is yellow-eyed\"]\n",
    "\n",
    "similarity = highlight_query(query, \"fused\", \"mean\", distilled_f, fused_f, filtered_pc, filtered_pc_c, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26700425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Blue-faced Honeyeater: black-masked, yellow-bellied, white-throated, white-eyed, yellow-breasted, black-billed, long-tailed, blue-faced, black-crowned, white-tipped.\n",
      "\n",
      "Diamond Firetail: red-headed, black-shouldered, white-rumped, grey-backed, white-bellied, yellow-breasted, orange-breasted, black-billed, red-tailed, black-winged.\n",
      "\n",
      "Mouse-colored Tyrannulet: olive-backed, yellow-breasted, white-bellied, grey-crowned, grey-tailed, white-eyed, yellow-throated, black-billed, white-winged, grey-breasted.\n"
     ]
    }
   ],
   "source": [
    "#parse descriptors from openai api with gpt\n",
    "_prompt = \"Generate 10 visual descriptors for each of the following categories, they are bird species: [Blue-faced Honeyeater, Diamond Firetail, Mouse-colored Tyrannulet]. The descriptors will be used for input queries for a CLIP model. The descriptors should be concise and distinct from the descriptors of the other classes. Do not focus on behavior, but purely on attributes which are recognizable by the CLIP model. The output should be in the following form as a string: *class*: *descriptor1*, *descriptor2*, etc.\"\n",
    "\n",
    "descriptors = descriptors_from_prompt(_prompt, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec1c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blue-faced Honeyeater': ['a bird which is black-masked.',\n",
       "  'a bird which is yellow-bellied.',\n",
       "  'a bird which is white-throated.',\n",
       "  'a bird which is white-eyed.',\n",
       "  'a bird which is yellow-breasted.',\n",
       "  'a bird which is black-billed.',\n",
       "  'a bird which is long-tailed.',\n",
       "  'a bird which is blue-faced.',\n",
       "  'a bird which is black-crowned.',\n",
       "  'a bird which is white-tipped..'],\n",
       " 'Diamond Firetail': ['a bird which is red-headed.',\n",
       "  'a bird which is black-shouldered.',\n",
       "  'a bird which is white-rumped.',\n",
       "  'a bird which is grey-backed.',\n",
       "  'a bird which is white-bellied.',\n",
       "  'a bird which is yellow-breasted.',\n",
       "  'a bird which is orange-breasted.',\n",
       "  'a bird which is black-billed.',\n",
       "  'a bird which is red-tailed.',\n",
       "  'a bird which is black-winged..'],\n",
       " 'Mouse-colored Tyrannulet': ['a bird which is olive-backed.',\n",
       "  'a bird which is yellow-breasted.',\n",
       "  'a bird which is white-bellied.',\n",
       "  'a bird which is grey-crowned.',\n",
       "  'a bird which is grey-tailed.',\n",
       "  'a bird which is white-eyed.',\n",
       "  'a bird which is yellow-throated.',\n",
       "  'a bird which is black-billed.',\n",
       "  'a bird which is white-winged.',\n",
       "  'a bird which is grey-breasted..']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7b70fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set descriptors if necessary\n",
    "descriptors = {\"mouse-colored tyrannulet\": [\"a bird which is black-winged\", \"a bird which is white-breasted\", \"a bird which is long-tailed\"],\n",
    "               \"diamond firetail\" :         [\"a bird which is red-breasted\", \"a bird which is white-eyed\", \"a bird which is black-tailed\"],\n",
    "               \"blue-faced honeyeater\":     [\"a bird which is blue-faced\", \"a bird which is hooked-beak\", \"a bird which is yellow-throated\"]\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197aae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANNET_LABELS_20 = ['wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa',\n",
    "                     'table', 'door', 'window', 'bookshelf', 'picture','counter', 'desk', 'curtain', 'refrigerator', 'shower curtain',\n",
    "                     'toilet', 'sink', 'bathtub', 'otherfurniture']\n",
    "UNKNOWN_ID = 255\n",
    "NO_FEATURE_ID = 256\n",
    "\n",
    "for key, value in descriptors.items():\n",
    "    SCANNET_LABELS_20.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e308209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 111.53it/s]\n"
     ]
    }
   ],
   "source": [
    "class_ious, class_accs, mean_iou, mean_acc = evaluate(SCANNET_LABELS_20, descriptors, \"fused\", \"max\" , distilled_f, fused_f, filtered_pc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d710be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the added object labels into 20 \n",
    "# gt_ids = np.where(np.logical_or(filtered_pc_labels == 21, filtered_pc_labels == 22), 20, filtered_pc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9c3c3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes                 IoU\n",
      "----------------------------\n",
      "wall                       :          0.60410           (  7991/13228 )\n",
      "floor                      :          0.80123           (  8344/10414 )\n",
      "cabinet                    :          0.48422           (  5722/11817 )\n",
      "bed                        :          0.56505           (  2810/4973  )\n",
      "chair error!\n",
      "sofa                       :          0.77087           (  4562/5918  )\n",
      "table                      :          0.18677           (  1347/7212  )\n",
      "door                       :          0.25350           (  1287/5077  )\n",
      "window                     :          0.65585           (   566/863   )\n",
      "bookshelf error!\n",
      "picture error!\n",
      "counter                    :          0.17139           (   272/1587  )\n",
      "desk                       :          0.01610           (    32/1987  )\n",
      "curtain                    :          0.64372           (  4712/7320  )\n",
      "refrigerator               :          0.66776           (  1015/1520  )\n",
      "shower curtain error!\n",
      "toilet                     :          0.45884           (   418/911   )\n",
      "sink                       :          0.37650           (   157/417   )\n",
      "bathtub error!\n",
      "otherfurniture             :          0.00960           (    43/4477  )\n",
      "Blue-faced Honeyeater             :          0.00096           (     3/3122  )\n",
      "Diamond Firetail             :          0.00000           (     0/1771  )\n",
      "Mouse-colored Tyrannulet             :          0.32673           (   495/1515  )\n"
     ]
    }
   ],
   "source": [
    "print_results(SCANNET_LABELS_20, class_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75e97f",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14cb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the preprocessed file path\n",
    "sample_path_0 = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/birds/scannet_3d/example/scene0000_00_vh_clean_2.pth\"\n",
    "#sample_path_1 = \"D:/AT3DCV_Data/Preprocessed_OpenScene/data/scannet_3d/train/scene0000_01_vh_clean_2.pth\"\n",
    "#sample_path_2 = \"D:/AT3DCV_Data/Preprocessed_OpenScene/data/scannet_3d/train/scene0000_02_vh_clean_2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de5eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_0 = torch.load(sample_path_0) # coords,colors,labels\n",
    "#sample_1 = torch.load(sample_path_1) # coords,colors,labels\n",
    "#sample_2 = torch.load(sample_path_2) # coords,colors,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d69828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86414"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d643bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating all of the partial point clouds of the same scene (they don't overlap perfectly)\n",
    "#sample_points = np.concatenate((sample_0[0], sample_1[0], sample_2[0]))\n",
    "#sample_colors = np.concatenate((sample_0[1], sample_1[1], sample_2[1]))\n",
    "\n",
    "# single partial point cloud\n",
    "sample_points  = sample_0[0]\n",
    "sample_colors = sample_0[1]\n",
    "sample_labels = sample_0[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6663b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to view original scene\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(sample_points))\n",
    "#original colors\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(sample_colors))\n",
    "#------\n",
    "#paint uniform\n",
    "#sample_paint_uniform = np.asarray([200,200,200])/255.0 #redish\n",
    "#pcd.paint_uniform_color(sample_paint_uniform)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5135af0",
   "metadata": {},
   "source": [
    "# load fused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "212cf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the fused feature path\n",
    "feature_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/birds/fused/scene0000_00_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7f5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = torch.load(feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b12fcc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86414])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[\"mask_full\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f474ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([82723, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[\"feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18fc2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices where the mask is True\n",
    "indices = torch.nonzero(feature[\"mask_full\"]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65077247",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_point_cloud = sample_points[indices, :]\n",
    "filtered_point_cloud_colors = sample_colors[indices, :]\n",
    "filtered_point_cloud_labels = sample_labels[indices]\n",
    "gt_ids = filtered_point_cloud_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d360ff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   5.,   6.,   7.,   8.,  11.,  12.,  13.,\n",
       "        14.,  16.,  17.,  19.,  20.,  21.,  22., 255.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(filtered_point_cloud_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aafeeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace every occurrence of 21 with 20 if necessary\n",
    "gt_ids= np.where(filtered_point_cloud_labels == 21.0, 20.0, filtered_point_cloud_labels)\n",
    "gt_ids= np.where(gt_ids == 22.0, 20.0, gt_ids)\n",
    "# gt_ids = filtered_point_cloud_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee71e533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   5.,   6.,   7.,   8.,  11.,  12.,  13.,\n",
       "        14.,  16.,  17.,  19.,  20.,  21.,  22., 255.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(gt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d482d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(gt_ids, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c5c60c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12425, 10081,  9583,  2948,  5098,  4428,  1414,   593,   462,\n",
       "        1891,  5271,  1404,   454,   362,   642,  1798,  1771,  1476,\n",
       "       20622])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb65e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5798734e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82723, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_point_cloud.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd5dbb",
   "metadata": {},
   "source": [
    "# using clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a01ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56a8260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# highlight with a threshold\n",
    "# type the query here \n",
    "query = [\"dragon\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_text_embeddings = []\n",
    "    for category in tqdm(query):\n",
    "        texts = clip.tokenize(category)  #tokenize\n",
    "        texts = texts.cuda()\n",
    "        text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embeddings.mean(dim=0)\n",
    "        text_embedding /= text_embedding.norm()\n",
    "        all_text_embeddings.append(text_embedding)\n",
    "\n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "\n",
    "# normalizing \n",
    "fused_f = (feature[\"feat\"]/(feature[\"feat\"].norm(dim=-1, keepdim=True)+1e-5)).half()\n",
    "# calculating similarity matrix\n",
    "# similarity_matrix = torch.matmul(feature[\"feat\"].cuda(), all_text_embeddings) # \n",
    "similarity_matrix = fused_f.cuda() @ all_text_embeddings    \n",
    "    \n",
    "# set higher to increase the certainty (not always correct)\n",
    "threshold_percentage = 0.9\n",
    "cap = similarity_matrix.max().item()\n",
    "found_indices = torch.nonzero(similarity_matrix > cap*threshold_percentage, as_tuple=False).squeeze().T[0]\n",
    "\n",
    "# creating pc\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud))\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud_colors))\n",
    "\n",
    "found_region = pcd.select_by_index(found_indices.tolist())\n",
    "found_region.paint_uniform_color([1.0, 0, 0]) # paint related points to red\n",
    "rest = pcd.select_by_index(found_indices.tolist(), invert=True)\n",
    "o3d.visualization.draw_geometries([rest,found_region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a3778aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# highlight with a heatmap\n",
    "# type the query here \n",
    "# query = [\"deathwing\"]\n",
    "# query = [\" a blue-faced, yellow-crowned, white-breasted, black-eyed, long-billed, hooked-beak, yellow-beaked, yellow-breasted, yellow-throated and black-tailed bird\"]\n",
    "\n",
    "# mouse-colored tyrannulet \n",
    "query = [[\"grey-bodied\",\"yellow-breasted\",\"black-crowned\",\n",
    "          \"white-eyed\",\"black-winged\",\"yellow-throated\",\n",
    "          \"white-breasted\",\"yellow-billed\",\"grey-headed\",\n",
    "          \"long-tailed\",\"bird\"]]\n",
    "\n",
    "# diamong firetail\n",
    "query = [[\"red-breasted\",\"black-crowned\",\"gold-winged\",\n",
    "          \"black-winged\",\"white-eyed\",\"yellow-billed\",\n",
    "          \"red-headed\",\"black-tailed\",\"long-tailed\",\n",
    "          \"white-breasted\",\"bird\"]]\n",
    "\n",
    "\n",
    "\n",
    "#query = [\"bird\"]\n",
    "#query = [[\"Mouse-colored Tyrannulet bird\"]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_text_embeddings = []\n",
    "    for category in tqdm(query):\n",
    "        texts = clip.tokenize(category)  #tokenize\n",
    "        texts = texts.cuda()\n",
    "        text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embeddings.mean(dim=0)\n",
    "        text_embedding /= text_embedding.norm()\n",
    "        all_text_embeddings.append(text_embedding)\n",
    "\n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "\n",
    "# normalizing \n",
    "fused_f = (feature[\"feat\"]/(feature[\"feat\"].norm(dim=-1, keepdim=True)+1e-5)).half()\n",
    "# calculating similarity matrix\n",
    "# similarity_matrix = torch.matmul(feature[\"feat\"].cuda(), all_text_embeddings) # \n",
    "similarity_matrix = fused_f.cuda() @ all_text_embeddings    \n",
    "\n",
    "# creating pc\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud))\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud_colors))\n",
    "\n",
    "# heatmap\n",
    "cmap = plt.get_cmap('cividis')\n",
    "\n",
    "# normalize the tensor to the range [0, 1]\n",
    "normalized_tensor = (similarity_matrix - torch.min(similarity_matrix)) / (torch.max(similarity_matrix) - torch.min(similarity_matrix))\n",
    "\n",
    "colors = cmap(normalized_tensor.detach().cpu().numpy().squeeze())\n",
    "pcd_heatmap = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd_heatmap.points = o3d.utility.Vector3dVector(pcd.points)\n",
    "pcd_heatmap.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "\n",
    "#transform heatmap to the side\n",
    "pcd_heatmap.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) + [0,10,0])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd, pcd_heatmap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4a00d",
   "metadata": {},
   "source": [
    "# mIoU evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d90b706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANNET_LABELS_20 = ['wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa',\n",
    "                     'table', 'door', 'window', 'bookshelf', 'picture','counter', 'desk', 'curtain', 'refrigerator', 'shower curtain',\n",
    "                     'toilet', 'sink', 'bathtub', 'otherfurniture']\n",
    "UNKNOWN_ID = 255\n",
    "NO_FEATURE_ID = 256\n",
    "\n",
    "SCANNET_LABELS_20.append(query[0])\n",
    "#SCANNET_LABELS_20.append(\"bird\")\n",
    "\n",
    "CLASS_LABELS = SCANNET_LABELS_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b1fb861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 52.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    label_embeds = []\n",
    "    for category in tqdm(SCANNET_LABELS_20):\n",
    "        texts = clip.tokenize(category)  #tokenize\n",
    "        texts = texts.cuda()\n",
    "        text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embeddings.mean(dim=0)\n",
    "        text_embedding /= text_embedding.norm()\n",
    "        label_embeds.append(text_embedding)\n",
    "\n",
    "    label_embeds = torch.stack(label_embeds, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d45e721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes          IoU\n",
      "----------------------------\n",
      "wall          : 0.60079   (  8020/13349 )\n",
      "floor         : 0.80275   (  8363/10418 )\n",
      "cabinet       : 0.49356   (  5866/11885 )\n",
      "bed           : 0.81338   (  2820/3467  )\n",
      "chair error!\n",
      "sofa          : 0.79770   (  4586/5749  )\n",
      "table         : 0.16482   (  1059/6425  )\n",
      "door          : 0.26445   (  1286/4863  )\n",
      "window        : 0.65738   (   566/861   )\n",
      "bookshelf error!\n",
      "picture error!\n",
      "counter       : 0.15129   (   300/1983  )\n",
      "desk          : 0.01605   (    32/1994  )\n",
      "curtain       : 0.67212   (  4928/7332  )\n",
      "refrigerator  : 0.66579   (  1014/1523  )\n",
      "shower curtain error!\n",
      "toilet        : 0.45985   (   418/909   )\n",
      "sink          : 0.25487   (   157/616   )\n",
      "bathtub error!\n",
      "otherfurniture: 0.01146   (    49/4274  )\n",
      "bird          : 0.00000   (     0/1301  )\n"
     ]
    }
   ],
   "source": [
    "print('classes          IoU')\n",
    "print('----------------------------')\n",
    "for i in range(N_CLASSES):\n",
    "    label_name = CLASS_LABELS[i]\n",
    "    if not isinstance(label_name, str): label_name = target_label\n",
    "    try:\n",
    "        print('{0:<14s}: {1:>5.5f}   ({2:>6d}/{3:<6d})'.format(\n",
    "                label_name,\n",
    "                class_ious[label_name][0],\n",
    "                class_ious[label_name][1],\n",
    "                class_ious[label_name][2]))\n",
    "    except:\n",
    "        print(label_name + ' error!')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b60ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  engine=\"text-davinci-003\",\n",
    "  prompt=\"Could you generate 5 visual descriptors for each of the following object classes, they are bird species: [Blue-faced Honeyeate, Diamond Firetail, Mouse-colored Tyrannulet]. The descriptors will be used for input queries for a CLIP model. The descriptors should be concise and distinct from one another. Do not focus on behavior, but purely on attributes which are recognizable by the CLIP model. The output should be in the following form, without any additional text: object class 1, visual descriptor 1.1, visual descriptor 1.2\",\n",
    "\n",
    "  temperature=0.5,\n",
    "  max_tokens=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version of the aggregating text embeddings, it's not properly working\n",
    "def highlight_query(query, feature_type, model, distill, fused, fpc, fpcc, device):\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_text_embeddings = []\n",
    "        for category in tqdm(query):\n",
    "            texts = clip.tokenize(category)  #tokenize\n",
    "            texts = texts.to(device)\n",
    "            text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_embedding = text_embeddings.mean(dim=0)\n",
    "            text_embedding /= text_embedding.norm()\n",
    "            all_text_embeddings.append(text_embedding)\n",
    "\n",
    "        all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "\n",
    "        \n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ all_text_embeddings\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ all_text_embeddings\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ all_text_embeddings\n",
    "        pred_distill = distill.to(device) @ all_text_embeddings\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble @ all_text_embeddings\n",
    "        \n",
    "    print(similarity_matrix.shape)\n",
    "    # creating pc\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.asarray(fpc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.asarray(fpcc))\n",
    "\n",
    "    # heatmap\n",
    "    cmap = plt.get_cmap('cividis')\n",
    "\n",
    "    # normalize the tensor to the range [0, 1]\n",
    "    normalized_tensor = (similarity_matrix - torch.min(similarity_matrix)) / (torch.max(similarity_matrix) - torch.min(similarity_matrix))\n",
    "\n",
    "    colors = cmap(normalized_tensor.detach().cpu().numpy().squeeze())\n",
    "    pcd_heatmap = o3d.geometry.PointCloud()\n",
    "\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(pcd.points)\n",
    "    pcd_heatmap.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "\n",
    "    #transform heatmap to the side\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) + [0,10,0])\n",
    "\n",
    "    o3d.visualization.draw_geometries([pcd, pcd_heatmap])\n",
    "    \n",
    "def evaluate(labelset, descriptors, feature_type, model, distill, fused, gt_ids):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        label_embeds = []\n",
    "        for category in tqdm(labelset):\n",
    "            texts = clip.tokenize(category)  #tokenize\n",
    "            texts = texts.cuda()\n",
    "            text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_embedding = text_embeddings.mean(dim=0)\n",
    "            text_embedding /= text_embedding.norm()\n",
    "            label_embeds.append(text_embedding)\n",
    "\n",
    "        label_embeds = torch.stack(label_embeds, dim=1)\n",
    "        \n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ label_embeds\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ label_embeds\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ label_embeds\n",
    "        pred_distill = distill.to(device) @ label_embeds\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble.to(device) @ label_embeds\n",
    "        \n",
    "    pred_ids = torch.max(similarity_matrix, 1)[1].detach().cpu()    \n",
    "    \n",
    "    N_CLASSES = len(labelset)\n",
    "    confusion = confusion_matrix(pred_ids, gt_ids, N_CLASSES)\n",
    "    class_ious = {}\n",
    "    class_accs = {}\n",
    "    mean_iou = 0\n",
    "    mean_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(N_CLASSES):\n",
    "        label_name = labelset[i]\n",
    "\n",
    "        if not isinstance(label_name, str): \n",
    "            for key, value in descriptors.items():\n",
    "                if value == label_name:\n",
    "                    label_name = key\n",
    "                    \n",
    "        if (gt_ids==i).sum() == 0: # at least 1 point needs to be in the evaluation for this class\n",
    "            continue\n",
    "\n",
    "\n",
    "        class_ious[label_name] = get_iou(i, confusion)\n",
    "        class_accs[label_name] = class_ious[label_name][1] / (gt_ids==i).sum()\n",
    "        count+=1\n",
    "\n",
    "        mean_iou += class_ious[label_name][0]\n",
    "        mean_acc += class_accs[label_name]\n",
    "\n",
    "\n",
    "    mean_iou /= N_CLASSES\n",
    "    mean_acc /= N_CLASSES\n",
    "    \n",
    "    return class_ious, class_accs, mean_iou, mean_acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
