{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b9b9f8",
   "metadata": {},
   "source": [
    "# extended version of the `scannet_openseg.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fbaafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf2\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import io\n",
    "from os.path import join, exists\n",
    "from utils.fusion_util import PointCloudToImageMapper, save_fused_feature_no_args, read_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7234a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 1457\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "#!### Dataset specific parameters #####\n",
    "img_dim = (320, 240) # original images from ScanNet are (640, 480) but in the preprocess they resize  images to (320, 240)\n",
    "depth_scale = 1000.0\n",
    "#######################################\n",
    "visibility_threshold = 0.25 # threshold for the visibility check\n",
    "cut_num_pixel_boundary = 10  # do not use the features on the image boundary\n",
    "feat_dim = 768 # CLIP feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a9feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "if split== 'train': # for training set, export a chunk of point cloud\n",
    "    n_split_points = 20000\n",
    "    num_rand_file_per_scene = 5\n",
    "else: # for the validation set, export the entire point cloud instead of chunks\n",
    "    n_split_points = 2000000\n",
    "    num_rand_file_per_scene = 1\n",
    "\n",
    "DATA_DIR = \"D:/AT3DCV_Data/Preprocessed_OpenScene/data\"\n",
    "DATA_ROOT = join(DATA_DIR, 'scannet_3d')\n",
    "DATA_ROOT_2D = join(DATA_DIR,'scannet_2d')\n",
    "\n",
    "data_paths = sorted(glob(join(DATA_ROOT, split, '*.pth')))\n",
    "total_num = len(data_paths) # total number of samples in dataset\n",
    "\n",
    "OUT_DIR = \"D:/AT3DCV_Data/Preprocessed_OpenScene/data/scannet_fused_features\"\n",
    "\n",
    "#load openseg model\n",
    "model_path = \"C:/Users/aorhu/Masaüstü/AT3DCV/repo/openseg_model\"\n",
    "openseg_model = tf2.saved_model.load(model_path,tags=[tf.saved_model.tag_constants.SERVING],)\n",
    "text_emb = tf.zeros([1, 1, feat_dim]) # creating zero tensor for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d7148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load intrinsic parameter\n",
    "intrinsics=np.loadtxt(os.path.join(DATA_ROOT_2D, 'intrinsics.txt'))\n",
    "\n",
    "# calculate image pixel-3D points correspondances\n",
    "point2img_mapper = PointCloudToImageMapper(\n",
    "        image_dim=img_dim, intrinsics=intrinsics,\n",
    "        visibility_threshold=visibility_threshold,\n",
    "        cut_bound=cut_num_pixel_boundary)\n",
    "\n",
    "process_id_range = [\"0,100\"]  # to only process samples in this range\n",
    "id_range = None\n",
    "if process_id_range is not None:\n",
    "    id_range = [int(process_id_range[0].split(',')[0]), int(process_id_range[0].split(',')[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8227907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1201 [00:00<?, ?it/s]\n",
      "  0%|                                                                                          | 0/279 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▎                                                                                 | 1/279 [00:05<27:00,  5.83s/it]\u001b[A\n",
      "  1%|▌                                                                                 | 2/279 [00:10<24:51,  5.38s/it]\u001b[A\n",
      "  1%|▉                                                                                 | 3/279 [00:16<24:42,  5.37s/it]\u001b[A\n",
      "  1%|█▏                                                                                | 4/279 [00:22<26:10,  5.71s/it]\u001b[A\n",
      "  2%|█▍                                                                                | 5/279 [00:28<27:13,  5.96s/it]\u001b[A\n",
      "  2%|█▊                                                                                | 6/279 [00:35<27:28,  6.04s/it]\u001b[A\n",
      "  3%|██                                                                                | 7/279 [00:41<27:44,  6.12s/it]\u001b[A\n",
      "  3%|██▎                                                                               | 8/279 [00:47<27:35,  6.11s/it]\u001b[A\n",
      "  3%|██▋                                                                               | 9/279 [00:54<28:33,  6.35s/it]\u001b[A\n",
      "  4%|██▉                                                                              | 10/279 [01:06<29:51,  6.66s/it]\u001b[A\n",
      "  0%|                                                                                         | 0/1201 [01:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m mapping \u001b[38;5;241m=\u001b[39m mapping\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# fusion\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m feat_2d_3d \u001b[38;5;241m=\u001b[39m \u001b[43mfeat_2d\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m#has the shape [81369, 768]\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# counting the image numbers and corresponding points\u001b[39;00m\n\u001b[0;32m    111\u001b[0m counter[mask\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# looping over samples\n",
    "for i in tqdm(range(total_num)):\n",
    "    # check the given range for the samples\n",
    "    if id_range is not None and (i<id_range[0] or i>id_range[1]):\n",
    "        print('skip ', i, data_paths[i])\n",
    "        continue\n",
    "    \n",
    "    # extraction of the features starts here, not using the provided functions\n",
    "    # ------------------------------------------------------------------------\n",
    "    data_path = data_paths[i]\n",
    "    #scene_id might be different depending on the path string, should be like \"scene0000_00\"\n",
    "    scene_id = data_path.split('/')[-1].split('\\\\')[-1].split('_vh')[0]\n",
    "    \n",
    "    # load 3D data (point cloud)\n",
    "    locs_in = torch.load(data_path)[0]\n",
    "    n_points = locs_in.shape[0] # number of points\n",
    "    \n",
    "    n_interval = num_rand_file_per_scene\n",
    "    n_finished = 0\n",
    "    for n in range(n_interval):\n",
    "        if exists(join(OUT_DIR, scene_id +'_%d.pt'%(n))):\n",
    "            n_finished += 1\n",
    "            print(scene_id +'_%d.pt'%(n) + ' already done!')\n",
    "            continue\n",
    "    if n_finished == n_interval:\n",
    "        continue\n",
    "    \n",
    "    # short hand for processing 2D features\n",
    "    scene = join(DATA_ROOT_2D, scene_id)\n",
    "    img_dirs = sorted(glob(join(scene, 'color/*')), key=lambda x: int(os.path.basename(x)[:-4]))\n",
    "    num_img = len(img_dirs) # number of images that the scene have\n",
    "        \n",
    "    # creating tensors to keep features per 3D point\n",
    "    n_points_cur = n_points\n",
    "    counter = torch.zeros((n_points_cur, 1), device = device)\n",
    "    sum_features = torch.zeros((n_points_cur, feat_dim), device = device)\n",
    "\n",
    "    vis_id = torch.zeros((n_points_cur, num_img), dtype=int, device=device)\n",
    "    \n",
    "    # process images per scene and fuse 2D-3D features\n",
    "    for img_id, img_dir in enumerate(tqdm(img_dirs)):\n",
    "        # load pose\n",
    "        posepath = img_dir.replace('color', 'pose').replace('.jpg', '.txt')\n",
    "        pose = np.loadtxt(posepath)\n",
    "        # load depth and convert to meter\n",
    "        depth = imageio.v2.imread(img_dir.replace('color', 'depth').replace('jpg', 'png')) / depth_scale # (240, 320)\n",
    "        \n",
    "        # calculate the 3d-2d mapping based on the depth\n",
    "        mapping = np.ones([n_points, 4], dtype=int)\n",
    "        \"\"\"\n",
    "        :pose: 4 x 4\n",
    "        :locs_in: N x 3 format (point cloud)\n",
    "        :depth: H x W format\n",
    "        :return: mapping, N x 3 format, (H,W,mask)\n",
    "        \"\"\"\n",
    "        mapping[:, 1:4] = point2img_mapper.compute_mapping(pose, locs_in, depth)\n",
    "        if mapping[:, 3].sum() == 0: # no points corresponds to this image, skip\n",
    "            continue     \n",
    "        mapping = torch.from_numpy(mapping).to(device)\n",
    "        mask = mapping[:, 3]    # [number of points]\n",
    "        vis_id[:, img_id] = mask # masking the points corresponding to the image index of the scene, [number of points, num of images per scene]\n",
    "\n",
    "        # extraction of 2D features with OpenSeg\n",
    "        # load RGB image\n",
    "        np_image_string = read_bytes(img_dir) #read_bytes is a simple function to read images as bytes for OpenSeg\n",
    "        # run OpenSeg\n",
    "        '''\n",
    "        results is a dictionary that has = ['region_probs_', 'text_embedding', 'segm_proposal_feats',\n",
    "                                            'image_embedding_feat', 'pixel_pred_confidence', 'segm_confidence', \n",
    "                                            'segm_prediction', 'images', 'region_embeddings', 'region_probs',\n",
    "                                            'ppixel_ave_feat', 'ppixel_ave_feat_confidence', 'segm_confidence_rw',\n",
    "                                            'image', 'segm_prediction_rw', 'image_info', 'ppixel_ave_feat_pred',\n",
    "                                            'pixel_prediction', 'segm_proposal', 'region_logits']\n",
    "        check OpenSeg repo for more information\n",
    "        \n",
    "        '''\n",
    "        results = openseg_model.signatures['serving_default'](\n",
    "                        inp_image_bytes = tf.convert_to_tensor(np_image_string),\n",
    "                        inp_text_emb = text_emb)\n",
    "        \n",
    "        img_info = results['image_info']\n",
    "        \n",
    "        crop_sz = [\n",
    "            int(img_info[0, 0] * img_info[2, 0]),\n",
    "            int(img_info[0, 1] * img_info[2, 1])\n",
    "        ]\n",
    "        \n",
    "        # this parameter is True by default in the original code, regardless of it the shape will be same\n",
    "        regional_pool = True \n",
    "        if regional_pool:\n",
    "            image_embedding_feat = results['ppixel_ave_feat'][:, :crop_sz[0], :crop_sz[1]] # shape will be : (1, 480, 640, 768)\n",
    "        else:\n",
    "            image_embedding_feat = results['image_embedding_feat'][:, :crop_sz[0], :crop_sz[1]] # shape will be : (1, 480, 640, 768)\n",
    "        \n",
    "        # resizing with nearest neighbors to 240x320\n",
    "        img_size=[240, 320] # set to this in the original code\n",
    "        if img_size is not None:\n",
    "            feat_2d = tf.cast(tf.image.resize_nearest_neighbor(\n",
    "                image_embedding_feat, img_size, align_corners=True)[0], dtype=tf.float16).numpy()\n",
    "        else:\n",
    "            feat_2d = tf.cast(image_embedding_feat[[0]], dtype=tf.float16).numpy()\n",
    "        \n",
    "        # reshaping for the fusion\n",
    "        feat_2d = torch.from_numpy(feat_2d).permute(2, 0, 1)\n",
    "        #without this conversion, it gives error while indexing\n",
    "        mapping = mapping.to(int)\n",
    "        # fusion\n",
    "        feat_2d_3d = feat_2d[:, mapping[:, 1], mapping[:, 2]].permute(1, 0).to(device) #has the shape [81369, 768]\n",
    "        \n",
    "        # counting the image numbers and corresponding points\n",
    "        counter[mask!=0]+= 1\n",
    "        sum_features[mask!=0] += feat_2d_3d[mask!=0] #has the shape [81369, 768]    \n",
    "        \n",
    "    counter[counter==0] = 1e-5 # to prevent division by zero\n",
    "    # dividing sum of the features per point by how many times the points had effect on the feature extraction\n",
    "    feat_bank = sum_features/counter # [81369, 768]\n",
    "    point_ids = torch.unique(vis_id.nonzero(as_tuple = False)[:, 0])\n",
    "    \n",
    "    #saving the fused features of randomly chosen n_split_points to total of num_rand_file_per_scene files\n",
    "    save_fused_feature_no_args(feat_bank, point_ids, n_points, OUT_DIR, scene_id, num_rand_file_per_scene, n_split_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8632d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
