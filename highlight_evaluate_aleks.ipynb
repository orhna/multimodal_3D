{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37317be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from os.path import join, exists\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import copy\n",
    "from tabulate import tabulate\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scene(path, visualize = True):\n",
    "    \n",
    "    sample = torch.load(path)\n",
    "    sample_points  = sample[0]\n",
    "    sample_colors = sample[1]\n",
    "    sample_labels = sample[2]\n",
    "    \n",
    "    if visualize:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(np.asarray(sample_points))\n",
    "        pcd.colors = o3d.utility.Vector3dVector(np.asarray(sample_colors))\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "        \n",
    "    return sample_points, sample_colors, sample_labels\n",
    "\n",
    "def load_fused_features(path, sample_points, sample_colors, sample_labels):\n",
    "    \n",
    "    features = torch.load(path)\n",
    "    indices = torch.nonzero(features[\"mask_full\"]).squeeze()\n",
    "    filtered_point_cloud = sample_points[indices, :]\n",
    "    filtered_point_cloud_colors = sample_colors[indices, :]\n",
    "    filtered_point_cloud_labels = sample_labels[indices]\n",
    "    fused_features = (features[\"feat\"]/(features[\"feat\"].norm(dim=-1, keepdim=True)+1e-5))\n",
    "    \n",
    "    return fused_features, filtered_point_cloud, filtered_point_cloud_colors, filtered_point_cloud_labels, indices\n",
    "\n",
    "def load_distilled_features(path, indices):\n",
    "    \n",
    "    distilled = np.load(path)\n",
    "    #cast and normalize embeddings for distilled \n",
    "    distilled = distilled[indices, :]\n",
    "    distilled_t = torch.from_numpy(distilled).half()\n",
    "    distilled_f = (distilled_t/(distilled_t.norm(dim=-1, keepdim=True)+1e-5))\n",
    "    \n",
    "    return distilled_f\n",
    "\n",
    "def draw_improvement(sim_old, sim_new, fpc, scale=None):\n",
    "    improvement = sim_new - sim_old\n",
    "    \n",
    "    if scale is None:\n",
    "        scale = 2 * torch.max(improvement.max(), -improvement.min())\n",
    "    \n",
    "    print(f'Scaled by {scale}')\n",
    "    improvement = improvement / (2*scale) + 0.5\n",
    "\n",
    "    # heatmap\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    \n",
    "    colors = cmap(improvement.detach().cpu().numpy().squeeze())\n",
    "    pcd_heatmap = o3d.geometry.PointCloud()\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(np.asarray(fpc))\n",
    "    pcd_heatmap.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "    \n",
    "    #o3d.visualization.draw_plotly([pcd_heatmap])\n",
    "    o3d.visualization.draw_geometries([pcd_heatmap])\n",
    "\n",
    "def highlight_query(query, feature_type, agg_type, distill, fused, fpc, fpcc, device, draw=True, scale=1, \n",
    "                    quantile=0.5, norm=False):\n",
    "    \n",
    "    import clip\n",
    "    model, preprocess = clip.load(\"ViT-L/14@336px\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        per_descriptor_embeds = []\n",
    "        for descriptor in tqdm(query):\n",
    "            _prompt = descriptor\n",
    "            print(_prompt)\n",
    "            texts = clip.tokenize(_prompt)  #tokenize\n",
    "            texts = texts.to(device)\n",
    "            text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            per_descriptor_embeds.append(text_embeddings)\n",
    "\n",
    "        per_descriptor_embeds = torch.stack(per_descriptor_embeds, dim=1).squeeze()\n",
    "\n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ per_descriptor_embeds.T\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ per_descriptor_embeds.T\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ per_descriptor_embeds.T\n",
    "        pred_distill = distill.to(device) @ per_descriptor_embeds.T\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble.to(device) @ per_descriptor_embeds.T\n",
    "        \n",
    "    if similarity_matrix.ndim == 2:\n",
    "        if agg_type == \"mean\":\n",
    "            agg_sim_mat = torch.mean(similarity_matrix, dim=1)\n",
    "        elif agg_type == \"max\":\n",
    "            agg_sim_mat, _ = torch.max(similarity_matrix, dim=1)\n",
    "        elif agg_type == \"median\":\n",
    "            agg_sim_mat, _ = torch.median(similarity_matrix, dim=1)\n",
    "        elif agg_type == \"quantile\":\n",
    "            agg_sim_mat = torch.quantile(similarity_matrix.float(), quantile, dim=1)\n",
    "        elif agg_type == \"min\":\n",
    "            agg_sim_mat, _ = torch.min(similarity_matrix, dim=1)\n",
    "        elif agg_type == \"bare-overlay-weight-max\":\n",
    "            maximum, _ = similarity_matrix[:,1:].max(dim=0)\n",
    "            weight = maximum / maximum.sum()\n",
    "            agg_sim_mat = similarity_matrix[:,0] + weight[None,:] @ similarity_matrix[:,1:].T \n",
    "        elif agg_type == \"bare-overlay-mean\":\n",
    "            weight = None\n",
    "            agg_sim_mat = similarity_matrix[:,0] + similarity_matrix[:,1:].mean(axis=1)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    else: \n",
    "        agg_sim_mat = similarity_matrix\n",
    "        \n",
    "    if norm:\n",
    "        agg_sim_mat -= agg_sim_mat.mean()\n",
    "        \n",
    "    agg_sim_mat = agg_sim_mat.reshape(-1, 1)\n",
    "    \n",
    "    # creating pc\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.asarray(fpc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.asarray(fpcc))\n",
    "\n",
    "    # heatmap\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "\n",
    "    # normalize the tensor to the range [0, 1]\n",
    "    print(f'Min: {torch.min(agg_sim_mat)}')\n",
    "    print(f'Max: {torch.max(agg_sim_mat)}')\n",
    "    normalized_tensor = scale * agg_sim_mat + 0.5\n",
    "    #normalized_tensor = (agg_sim_mat - torch.min(agg_sim_mat)) / (torch.max(agg_sim_mat) - torch.min(agg_sim_mat))\n",
    "\n",
    "    colors = cmap(normalized_tensor.detach().cpu().numpy().squeeze())\n",
    "    pcd_heatmap = o3d.geometry.PointCloud()\n",
    "\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(pcd.points)\n",
    "    pcd_heatmap.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "\n",
    "    #transform heatmap to the side\n",
    "    \n",
    "    #pcd_heatmap.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) + [0,10,0])\n",
    "    #o3d.visualization.draw_geometries([pcd, pcd_heatmap])\n",
    "    \n",
    "    pcd_heatmap.points = pcd.points\n",
    "    if draw:\n",
    "        #o3d.visualization.draw_plotly([pcd_heatmap])\n",
    "        o3d.visualization.draw_geometries([pcd_heatmap])\n",
    "    \n",
    "    return agg_sim_mat\n",
    "\n",
    "def confusion_matrix(pred_ids, gt_ids, num_classes):\n",
    "    '''calculate the confusion matrix.'''\n",
    "\n",
    "    assert pred_ids.shape == gt_ids.shape, (pred_ids.shape, gt_ids.shape)\n",
    "    idxs = gt_ids != UNKNOWN_ID\n",
    "    if NO_FEATURE_ID in pred_ids: # some points have no feature assigned for prediction\n",
    "        print(\"no features\")\n",
    "        pred_ids[pred_ids==NO_FEATURE_ID] = num_classes\n",
    "        confusion = np.bincount(\n",
    "            pred_ids[idxs] * (num_classes+1) + gt_ids[idxs],\n",
    "            minlength=(num_classes+1)**2).reshape((\n",
    "            num_classes+1, num_classes+1)).astype(np.ulonglong)\n",
    "        return confusion[:num_classes, :num_classes]\n",
    "\n",
    "    return np.bincount(\n",
    "        pred_ids[idxs] * num_classes + gt_ids[idxs],\n",
    "        minlength=num_classes**2).reshape((\n",
    "        num_classes, num_classes)).astype(np.ulonglong)\n",
    "\n",
    "def get_iou(label_id, confusion, gts):\n",
    "    '''calculate IoU.'''\n",
    "\n",
    "    # true positives\n",
    "    tp = np.longlong(confusion[label_id, label_id])\n",
    "    # false positives\n",
    "    fp = np.longlong(confusion[label_id, :].sum()) - tp\n",
    "    # false negatives\n",
    "    fn = np.longlong(confusion[:, label_id].sum()) - tp\n",
    "\n",
    "    total = np.sum(gts == label_id)\n",
    "    denom = (tp + fp + fn)\n",
    "    if denom == 0:\n",
    "        return float('nan')\n",
    "    return float(tp) / denom, tp, denom, total\n",
    "\n",
    "def evaluate(labelset, descriptors, feature_type, agg_type, distill, fused, gt_ids):\n",
    "    import clip\n",
    "    model, preprocess = clip.load(\"ViT-L/14@336px\")\n",
    "    \n",
    "    descriptor_lengths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        label_embeds = []\n",
    "        for category in labelset:\n",
    "            if not isinstance(category, str): # if not string, process in another loop\n",
    "                descriptor_lengths.append(len(category)) # get length of descriptors\n",
    "                for desc in category:\n",
    "                    texts = clip.tokenize(desc)  #tokenize\n",
    "                    texts = texts.cuda()\n",
    "                    text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "                    text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "                    label_embeds.append(text_embeddings)\n",
    "            else: # if string, just process\n",
    "                _prompt = f'a {category} in a scene' \n",
    "                texts = clip.tokenize(_prompt)  #tokenize\n",
    "                texts = texts.cuda()\n",
    "                text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "                text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "                label_embeds.append(text_embeddings)\n",
    "                \n",
    "    label_embeds = torch.cat(label_embeds, dim=0) # has the shape of [768, *original labels*+*descriptors*]\n",
    "\n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ label_embeds.T\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ label_embeds.T\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ label_embeds.T\n",
    "        pred_distill = distill.to(device) @ label_embeds.T\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble.to(device) @ label_embeds.T\n",
    "    \n",
    "    # separating the similarity matrix for the original labels and descriptors, 20 only for ScanNet\n",
    "    sim_labels = similarity_matrix[:, :20]\n",
    "    sim_descriptors = similarity_matrix[:, 20:]\n",
    "            \n",
    "    # aggregate the corresponding descriptor vectors by the length of each\n",
    "    # keep them in a list to stack it later\n",
    "    _idx = 0\n",
    "    agg_desc = []\n",
    "    for elem in descriptor_lengths:\n",
    "        sim_descriptor = sim_descriptors[:, _idx : _idx + elem]\n",
    "        if agg_type == \"mean\":\n",
    "            agg_desc_sim_mat = torch.mean(sim_descriptor, dim=1)\n",
    "        elif agg_type == \"max\":\n",
    "            agg_desc_sim_mat, _ = torch.max(sim_descriptor, dim=1)\n",
    "        elif agg_type == \"median\":\n",
    "            agg_desc_sim_mat, _ = torch.median(sim_descriptor, dim=1)\n",
    "        elif agg_type == \"min\":\n",
    "            agg_desc_sim_mat, _ = torch.min(sim_descriptor, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        _idx += elem\n",
    "        agg_desc.append(agg_desc_sim_mat)\n",
    "        \n",
    "    # stack aggregated descriptor similarity matrices\n",
    "    agg_sim_descriptors = torch.stack(agg_desc, dim = 1)\n",
    "    # combine with the similarity matrix of labels\n",
    "    agg_sim_mat = torch.cat([sim_labels,agg_sim_descriptors], dim = 1)\n",
    "\n",
    "    # get the predictions\n",
    "    pred_ids = torch.max(agg_sim_mat, 1)[1].detach().cpu()    \n",
    "    \n",
    "    N_CLASSES = len(labelset)\n",
    "    \n",
    "    confusion = confusion_matrix(pred_ids, gt_ids, N_CLASSES)\n",
    "    class_ious = {}\n",
    "    class_accs = {}\n",
    "    mean_iou = 0\n",
    "    mean_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(N_CLASSES):\n",
    "        label_name = labelset[i]\n",
    "        \n",
    "        if not isinstance(label_name, str): \n",
    "            for key, value in descriptors.items():\n",
    "                if value == label_name:\n",
    "                    label_name = key\n",
    "        if (gt_ids==i).sum() == 0: # at least 1 point needs to be in the evaluation for this class\n",
    "            continue\n",
    "            \n",
    "        class_ious[label_name] = get_iou(i, confusion, gt_ids)\n",
    "        class_accs[label_name] = class_ious[label_name][1] / (gt_ids==i).sum()\n",
    "        count+=1\n",
    "\n",
    "        mean_iou += class_ious[label_name][0]\n",
    "        mean_acc += class_accs[label_name]\n",
    "\n",
    "    mean_iou /= N_CLASSES\n",
    "    mean_acc /= N_CLASSES\n",
    "    \n",
    "    return class_ious, class_accs, mean_iou, mean_acc, confusion\n",
    "\n",
    "def print_results(labelset, class_ious, descriptors):\n",
    "    \n",
    "    print('classes                 IoU/ total')\n",
    "    print('----------------------------')\n",
    "    for i in range(len(labelset)):\n",
    "        label_name = labelset[i]\n",
    "        if not isinstance(label_name, str): \n",
    "            for key, value in descriptors.items():\n",
    "                if value == label_name:\n",
    "                    label_name = key\n",
    "        try:\n",
    "            print('{0:<14s}             :          {1:>5.5f}           ({2:>6d}/{3:<6d}   /{4:<6d})'.format(\n",
    "                    label_name,\n",
    "                    class_ious[label_name][0],\n",
    "                    class_ious[label_name][1],\n",
    "                    class_ious[label_name][2],\n",
    "                    class_ious[label_name][3]))\n",
    "        except:\n",
    "            print(label_name + ' error!')\n",
    "            continue\n",
    "            \n",
    "def print_results_table(labelset, class_ious, descriptors):\n",
    "    \n",
    "    results = [[\"classes\",\"IoU\", \"tp/(tp + fp + fn)\", \"total points\" ]]\n",
    "    \n",
    "    for i in range(len(labelset)):\n",
    "        label_name = labelset[i]\n",
    "        if not isinstance(label_name, str): \n",
    "            for key, value in descriptors.items():\n",
    "                if value == label_name:\n",
    "                    label_name = key\n",
    "        try:\n",
    "            results.append([label_name,\n",
    "                            format(class_ious[label_name][0], '.5f'),\n",
    "                            f'{class_ious[label_name][1]}/{class_ious[label_name][2]}',\n",
    "                            class_ious[label_name][3]])\n",
    "        except:\n",
    "            results.append([label_name, \"---\", \"---\",\"---\"])        \n",
    "            continue\n",
    "            \n",
    "        table = tabulate(results, headers=\"firstrow\", tablefmt=\"rounded_outline\")\n",
    "        \n",
    "    print(table)\n",
    "        \n",
    "def descriptors_from_prompt(text, verbose = True):\n",
    "    \n",
    "    import openai\n",
    "\n",
    "    openai.api_key = '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'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=text,\n",
    "\n",
    "      temperature=0.5,\n",
    "      max_tokens=200\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(response[\"choices\"][0].text)\n",
    "    \n",
    "    lines = [s for s in [line.strip() for line in response[\"choices\"][0].text.splitlines()] if s]\n",
    "    \n",
    "    descriptors = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.split(\":\")\n",
    "        key = parts[0].strip()\n",
    "        features = [f'a bird which has {f.strip()}.' for f in parts[1].split(\",\")]\n",
    "        descriptors[key] = features\n",
    "        \n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def combinations_descriptor(descriptors, subset_len):\n",
    "    \n",
    "    combinations_dict= {}\n",
    "    for key, value in descriptors.items():\n",
    "        value_combinations = list(combinations(value, subset_len))\n",
    "        combinations_dict[key] = value_combinations\n",
    "    \n",
    "    comb_dict_list = []\n",
    "    for i in range(len(combinations_dict[next(iter(combinations_dict))])):\n",
    "        temp = {}\n",
    "        for key in combinations_dict.keys():\n",
    "            temp[key] = [str(item) for item in combinations_dict[key][i]]\n",
    "        comb_dict_list.append(temp)\n",
    "        \n",
    "    return comb_dict_list\n",
    "\n",
    "def try_diff_combs(labelset, comb_dict_list, feature_type, agg_type, distill, fused, gt_ids):\n",
    "\n",
    "    class_IoU_result_list = []\n",
    "    class_accs_result_list = []\n",
    "    mean_iou_result_list = []\n",
    "    mean_acc_result_list = []\n",
    "    \n",
    "    for elem in tqdm(comb_dict_list):\n",
    "        temp_labelset = copy.deepcopy(labelset)\n",
    "        \n",
    "        for key, value in elem.items():\n",
    "            temp_labelset.append(value)\n",
    "            \n",
    "        class_ious, class_accs, mean_iou, mean_acc = evaluate(temp_labelset, elem, feature_type, agg_type , distill, fused, gt_ids)\n",
    "        class_IoU_result_list.append(class_ious)\n",
    "        class_accs_result_list.append(class_accs)\n",
    "        mean_iou_result_list.append(mean_iou)\n",
    "        mean_acc_result_list.append(mean_acc)\n",
    "    \n",
    "    return class_IoU_result_list, class_accs_result_list, mean_iou_result_list, mean_acc_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the required data\n",
    "#source_path = \"/home/aleks/3dcv/openseg_aug/chair_scene_alex/scannet_3d/example/scene0000_00_vh_clean_2.pth\"\n",
    "#fused_path = \"/home/aleks/3dcv/openseg_aug/chair_scene_alex/fused/scene0000_00_0.pt\"\n",
    "#distilled_path = \"/home/aleks/3dcv/openseg_aug/chair_scene_alex/features_3D/scene0000_00_vh_clean_2_openscene_feat_distill.npy\"\n",
    "\n",
    "source_path = \"/home/aleks/3dcv/openseg_aug_new/scannet_3d/example/scene0000_00_vh_clean_2.pth\"\n",
    "fused_path = \"/home/aleks/3dcv/openseg_aug_new/fused/scene0000_00_0.pt\"\n",
    "distilled_path = \"/home/aleks/3dcv/openseg_aug_new/features_3D/scene0000_00_vh_clean_2_openscene_feat_distill.npy\"\n",
    "\n",
    "source_points, source_colors, source_labels = load_scene(source_path, False)\n",
    "\n",
    "fused_f, filtered_pc, filtered_pc_c, filtered_pc_labels, indices = load_fused_features(fused_path,\n",
    "                                                                              source_points, \n",
    "                                                                              source_colors,\n",
    "                                                                              source_labels)\n",
    "distilled_f = load_distilled_features(distilled_path, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = {\n",
    "    \"stool\": [\n",
    "        \"Stool with backless\",\n",
    "        \"Backless stool\",\n",
    "        \"Stool with cylindrical legs\",\n",
    "        \"Small stool\",\n",
    "        \"Stool with flat seat\",\n",
    "        \"Simple stool\",\n",
    "        \"Stool with no armrests\",\n",
    "        \"Wooden stool\",\n",
    "        \"Stool with round seat\",\n",
    "        \"Metal stool\",\n",
    "    ],\n",
    "    \"armchair\": [\n",
    "        \"Armchair with padded arms\",\n",
    "        \"Comfy armchair\",\n",
    "        \"Armchair with high back\",\n",
    "        \"Upholstered armchair\",\n",
    "        \"Armchair with wooden frame\",\n",
    "        \"Leather armchair\",\n",
    "        \"Armchair with cushioned seat\",\n",
    "        \"Modern armchair\",\n",
    "        \"Armchair with decorative legs\",\n",
    "        \"Vintage armchair\",\n",
    "    ],\n",
    "    \"rocking chair\": [\n",
    "        \"Rocking chair with curved runners\",\n",
    "        \"Wooden rocking chair\",\n",
    "        \"Rocking chair with slat back\",\n",
    "        \"Outdoor rocking chair\",\n",
    "        \"Upholstered rocking chair\",\n",
    "        \"Rocking chair with armrests\",\n",
    "        \"Classic rocking chair\",\n",
    "        \"Rocking chair with wide seat\",\n",
    "        \"Modern rocking chair\",\n",
    "        \"Nursery rocking chair\",\n",
    "    ],\n",
    "    \"ball chair\": [\n",
    "        \"Round ball chair\",\n",
    "        \"Modern ball chair\",\n",
    "        \"High back ball chair\",\n",
    "        \"Hanging ball chair\",\n",
    "        \"Swivel base ball chair\",\n",
    "        \"Outdoor ball chair\",\n",
    "        \"Padded seat ball chair\",\n",
    "        \"Enclosed ball chair\",\n",
    "        \"Sphere-shaped ball chair\",\n",
    "        \"Retro ball chair\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_bare = {\n",
    "    \"stool\": [\"backless\", \"compact\", \"straight legs\", \"round seat\", \"simple design\", \n",
    "              \"no armrests\", \"low height\", \"versatile\", \"footrest\", \"wooden\"],\n",
    "    \n",
    "    \"armchair\": [\"upholstered\", \"armrests\", \"comfortable\", \"cushioned\", \"high back\", \n",
    "                 \"wingback\", \"padded seat\", \"elegant design\", \"curved legs\", \"fabric\"],\n",
    "    \n",
    "    \"rocking chair\": [\"curved runners\", \"sloping back\", \"rocking motion\", \"wooden frame\", \"comfortable seat\", \n",
    "                      \"traditional design\", \"armrests\", \"relaxing\", \"classic\", \"country-style\"],\n",
    "    \n",
    "    \"ball chair\": [\"sphere-shaped\", \"modern\", \"futuristic\", \"enclosed space\", \"swivel base\", \n",
    "                   \"comfortable cushion\", \"unique design\", \"bubble chair\", \"acrylic material\", \"iconic\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANNET_LABELS_20 = ['wall', 'floor', 'cabinet', 'bed', 'chair', \n",
    "                     'sofa', 'table', 'door', 'window', 'bookshelf', \n",
    "                     'picture','counter', 'desk', 'curtain', 'refrigerator', \n",
    "                     'shower curtain', 'toilet', 'sink', 'bathtub', 'otherfurniture']\n",
    "\n",
    "SCANNET_LABELS_AUG = SCANNET_LABELS_20 + list(descriptors.keys())\n",
    "\n",
    "UNKNOWN_ID = 255\n",
    "NO_FEATURE_ID = 256\n",
    "\n",
    "agg_type = 'mean'\n",
    "feature_type = 'fused'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = 'ensembled'\n",
    "obj_class = 'ball chair'\n",
    "\n",
    "sim_class = highlight_query(['armchair'], feature_type, 'mean', distilled_f, fused_f, filtered_pc, \n",
    "                            filtered_pc_c, device, draw=True, scale=2.)\n",
    "\n",
    "for d in descriptors['armchair']:\n",
    "    sim_class = highlight_query([d], feature_type, 'mean', distilled_f, fused_f, filtered_pc, \n",
    "                                filtered_pc_c, device, draw=True, scale=2.)\n",
    "\n",
    "#sim_desc  = highlight_query(descriptors['armchair'], feature_type, 'mean', \n",
    "#                            distilled_f, fused_f, filtered_pc, filtered_pc_c, device, draw=True, scale=2.)\n",
    "\n",
    "sim_desc  = highlight_query(descriptors_bare[obj_class], feature_type, 'quantile', \n",
    "                            distilled_f, fused_f, filtered_pc, filtered_pc_c, device, draw=True, scale=5.,\n",
    "                            quantile=0.4, norm=True)\n",
    "\n",
    "#sim_desc  = highlight_query([obj_class] + descriptors_bare[obj_class], feature_type, 'bare-overlay-mean', \n",
    "#                            distilled_f, fused_f, filtered_pc, filtered_pc_c, device, draw=True, scale=2.)\n",
    "\n",
    "draw_improvement(sim_class, sim_desc, filtered_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity_results = {}\n",
    "#difference_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in SCANNET_LABELS_AUG:\n",
    "    if class_name not in similarity_results.keys():\n",
    "        similarity_results[class_name] = highlight_query([class_name], feature_type, agg_type, \n",
    "                                                         distilled_f, fused_f, filtered_pc, \n",
    "                                                         filtered_pc_c, device, draw=False)\n",
    "\n",
    "for class_name, descriptor_list in descriptors.items():\n",
    "    for descriptor in descriptor_list:\n",
    "        if descriptor not in similarity_results.keys():\n",
    "            similarity = highlight_query([descriptor], feature_type, agg_type, \n",
    "                                         distilled_f, fused_f, filtered_pc, \n",
    "                                         filtered_pc_c, device, draw=False)\n",
    "            similarity_results[descriptor] = similarity\n",
    "            difference_results[descriptor] = similarity - similarity_results[class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9207b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity_results_bare = {}\n",
    "#difference_results_bare = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447928b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for class_name in SCANNET_LABELS_AUG:\n",
    "    if class_name not in similarity_results_bare.keys():\n",
    "        similarity_results_bare[class_name] = similarity_results[class_name]\n",
    "\n",
    "for class_name, descriptor_list in descriptors_bare.items():\n",
    "    for descriptor in descriptor_list:\n",
    "        if descriptor not in similarity_results_bare.keys():\n",
    "            similarity = highlight_query([descriptor], feature_type, agg_type, \n",
    "                                         distilled_f, fused_f, filtered_pc, \n",
    "                                         filtered_pc_c, device, draw=False)\n",
    "            similarity_results_bare[descriptor] = similarity\n",
    "            difference_results_bare[descriptor] = similarity - similarity_results_bare[class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f03ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for obj_class, descs_class in descriptors.items():\n",
    "    gt_label = SCANNET_LABELS_AUG.index(obj_class)\n",
    "    other_chair_mask = np.all([filtered_pc_labels != gt_label, \n",
    "                               20 <= filtered_pc_labels, \n",
    "                               filtered_pc_labels < len(SCANNET_LABELS_AUG)], axis=0)\n",
    "    \n",
    "    y_labels = []\n",
    "    x_similarity_other_general = []\n",
    "    x_difference_other_general = []\n",
    "    \n",
    "    x_similarity_other_chair = []\n",
    "    x_difference_other_chair = []\n",
    "    \n",
    "    \n",
    "    sim = similarity_results[obj_class]\n",
    "        \n",
    "    avg_similarity_class = sim[filtered_pc_labels == gt_label].mean().cpu()\n",
    "    avg_similarity_other_chair = sim[other_chair_mask].mean().cpu()\n",
    "    avg_similarity_other_general = sim[filtered_pc_labels != gt_label].mean().cpu()\n",
    "\n",
    "    y_labels.append(obj_class)\n",
    "\n",
    "    x_similarity_other_general.append(avg_similarity_class-avg_similarity_other_general)\n",
    "    x_similarity_other_chair.append(avg_similarity_class-avg_similarity_other_chair)\n",
    "    \n",
    "    for desc in descs_class:\n",
    "        sim = similarity_results[desc]\n",
    "        \n",
    "        avg_similarity_class = sim[filtered_pc_labels == gt_label].mean().cpu()\n",
    "        avg_similarity_other_chair = sim[other_chair_mask].mean().cpu()\n",
    "        avg_similarity_other_general = sim[filtered_pc_labels != gt_label].mean().cpu()\n",
    "        \n",
    "        diff = difference_results[desc]\n",
    "        \n",
    "        avg_difference_class = diff[filtered_pc_labels == gt_label].mean().cpu()\n",
    "        avg_difference_other_chair = diff[other_chair_mask].mean().cpu()\n",
    "        avg_difference_other_general = diff[filtered_pc_labels != gt_label].mean().cpu()\n",
    "        \n",
    "        y_labels.append(desc)\n",
    "        \n",
    "        x_similarity_other_general.append(avg_similarity_class-avg_similarity_other_general)\n",
    "        x_difference_other_general.append(avg_difference_class-avg_difference_other_general)\n",
    "        x_similarity_other_chair.append(avg_similarity_class-avg_similarity_other_chair)\n",
    "        x_difference_other_chair.append(avg_difference_class-avg_difference_other_chair)\n",
    "\n",
    "    sorted_indices = sorted(range(len(y_labels)), key=lambda i: x_similarity_other_chair[i])\n",
    "        \n",
    "    plt.figure(figsize=(8,4))\n",
    "    Y_axis = np.arange(len(y_labels))\n",
    "    plt.barh(Y_axis-0.2, [x_similarity_other_general[i] for i in sorted_indices], 0.4, color='blue',\n",
    "             label='to other objects')\n",
    "    plt.barh(Y_axis+0.2, [x_similarity_other_chair[i] for i in sorted_indices], 0.4, color='orange',\n",
    "             label='to other chairs')\n",
    "    plt.yticks(Y_axis, [y_labels[i] for i in sorted_indices])\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1))\n",
    "    plt.title(f'Average similarity difference of \\'{obj_class}\\'')\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc97d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for obj_class, descs_class in descriptors_bare.items():\n",
    "    gt_label = SCANNET_LABELS_AUG.index(obj_class)\n",
    "    other_chair_mask = np.all([filtered_pc_labels != gt_label, \n",
    "                               20 <= filtered_pc_labels, \n",
    "                               filtered_pc_labels < len(SCANNET_LABELS_AUG)], axis=0)\n",
    "    \n",
    "    y_labels = []\n",
    "    x_similarity_other_general = []\n",
    "    x_difference_other_general = []\n",
    "    \n",
    "    x_similarity_other_chair = []\n",
    "    x_difference_other_chair = []\n",
    "    \n",
    "    \n",
    "    sim = similarity_results_bare[obj_class]\n",
    "        \n",
    "    avg_similarity_class = sim[filtered_pc_labels == gt_label].mean().cpu()\n",
    "    avg_similarity_other_chair = sim[other_chair_mask].mean().cpu()\n",
    "    avg_similarity_other_general = sim[filtered_pc_labels != gt_label].mean().cpu()\n",
    "\n",
    "    y_labels.append(obj_class)\n",
    "\n",
    "    x_similarity_other_general.append(avg_similarity_class-avg_similarity_other_general)\n",
    "    x_similarity_other_chair.append(avg_similarity_class-avg_similarity_other_chair)\n",
    "    \n",
    "    for desc in descs_class:\n",
    "        sim = similarity_results_bare[desc]\n",
    "        \n",
    "        avg_similarity_class = sim[filtered_pc_labels == gt_label].mean().cpu()\n",
    "        avg_similarity_other_chair = sim[other_chair_mask].mean().cpu()\n",
    "        avg_similarity_other_general = sim[filtered_pc_labels != gt_label].mean().cpu()\n",
    "        \n",
    "        diff = difference_results_bare[desc]\n",
    "        \n",
    "        avg_difference_class = diff[filtered_pc_labels == gt_label].mean().cpu()\n",
    "        avg_difference_other_chair = diff[other_chair_mask].mean().cpu()\n",
    "        avg_difference_other_general = diff[filtered_pc_labels != gt_label].mean().cpu()\n",
    "        \n",
    "        y_labels.append(desc)\n",
    "        \n",
    "        x_similarity_other_general.append(avg_similarity_class-avg_similarity_other_general)\n",
    "        x_difference_other_general.append(avg_difference_class-avg_difference_other_general)\n",
    "        x_similarity_other_chair.append(avg_similarity_class-avg_similarity_other_chair)\n",
    "        x_difference_other_chair.append(avg_difference_class-avg_difference_other_chair)\n",
    "\n",
    "    sorted_indices = sorted(range(len(y_labels)), key=lambda i: x_similarity_other_chair[i])\n",
    "        \n",
    "    plt.figure(figsize=(8,4))\n",
    "    Y_axis = np.arange(len(y_labels))\n",
    "    plt.barh(Y_axis-0.2, [x_similarity_other_general[i] for i in sorted_indices], 0.4, \n",
    "             label='to other objects', color='blue')\n",
    "    plt.barh(Y_axis+0.2, [x_similarity_other_chair[i] for i in sorted_indices], 0.4, \n",
    "             label='to other chairs', color='orange')\n",
    "    plt.yticks(Y_axis, [y_labels[i] for i in sorted_indices])\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1))\n",
    "    plt.title(f'Average similarity difference of \\'{obj_class}\\'')\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd9c3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _descriptors in various_descriptors:\n",
    "    \n",
    "    SCANNET_LABELS_DESC = SCANNET_LABELS_20.copy()\n",
    "    for key, value in _descriptors.items():\n",
    "        SCANNET_LABELS_DESC.append(value)\n",
    "        \n",
    "    print(SCANNET_LABELS_DESC)\n",
    "    print(f'Aggregation: {agg_type}')\n",
    "    print(f'Features: {feature_type}')\n",
    "        \n",
    "    class_ious, class_accs, mean_iou, mean_acc, confusion = \\\n",
    "        evaluate(SCANNET_LABELS_DESC, _descriptors, feature_type, agg_type , distilled_f, fused_f, filtered_pc_labels)\n",
    "    \n",
    "    print(f'Mean acc: {mean_acc}')\n",
    "    \n",
    "    col_sums = confusion.sum(axis=0)\n",
    "    col_sums[col_sums==0] = 1\n",
    "    confusion = confusion / col_sums[np.newaxis, :]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    #ax.set_xticks(np.arange(len(SCANNET_LABELS_AUG)), labels=SCANNET_LABELS_AUG)\n",
    "    ax.set_yticks(np.arange(len(SCANNET_LABELS_AUG)), labels=SCANNET_LABELS_AUG)\n",
    "    #plt.setp([tick.label for tick in ax.xaxis.get_major_ticks()], rotation=45, ha=\"left\",\n",
    "    #     rotation_mode=\"anchor\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print_results_table(SCANNET_LABELS_DESC, class_ious, _descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc957265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_table(SCANNET_LABELS_20, class_ious, descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84208e29",
   "metadata": {},
   "source": [
    "# experiment with different descriptor combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse descriptors from openai api with gpt, set _nr to the number of descriptors you'd want to retrieve\n",
    "_nr = 10\n",
    "_prompt = f'Generate {str(_nr)} visual descriptors for each of the following categories, they are bird species: [Blue-faced Honeyeater, Diamond Firetail, Mouse-colored Tyrannulet]. The descriptors will be used for input queries for a CLIP model. The descriptors should be concise and distinct from the descriptors of the other classes. Do not focus on behavior, but purely on attributes which are recognizable by the CLIP model. The output should be in the following form as a string: *class*: *descriptor1*, *descriptor2*, etc.'\n",
    "descriptors = descriptors_from_prompt(_prompt, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e878b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if retrieved descriptors are in proper shape\n",
    "descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ca0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANNET_LABELS_20 = ['wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa',\n",
    "                     'table', 'door', 'window', 'bookshelf', 'picture','counter', 'desk', 'curtain', 'refrigerator', 'shower curtain',\n",
    "                     'toilet', 'sink', 'bathtub', 'otherfurniture']\n",
    "UNKNOWN_ID = 255\n",
    "NO_FEATURE_ID = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get combinations of 5 out of *_nr* descriptors for each class\n",
    "comb_dict_list = combinations_descriptor(descriptors, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1027f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate all the combinations and store their results in a list\n",
    "class_IoU_result_list, class_accs_result_list, mean_iou_result_list, mean_acc_result_list = try_diff_combs(SCANNET_LABELS_20, comb_dict_list, \"fused\", \"mean\", distilled_f, fused_f, filtered_pc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0th index of the list corresponds to the descriptors that belongs to the 0th index of the comb_dict_list\n",
    "class_IoU_result_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96924aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2, c3 = [],[],[], # Blue-faced Honeyeater, Diamond Firetail, Mouse-colored Tyrannulet\n",
    "\n",
    "# store tp/ (tp + fp + fn) values in list per augmented class\n",
    "for idx in range(len(class_IoU_result_list)):\n",
    "    c1.append(class_IoU_result_list[idx][\"Blue-faced Honeyeater\"][0])\n",
    "    c2.append(class_IoU_result_list[idx][\"Diamond Firetail\"][0])\n",
    "    c3.append(class_IoU_result_list[idx][\"Mouse-colored Tyrannulet\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eec7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these 5 descriptors gives the highest class IoU for Mouse-colored Tyrannulet\n",
    "comb_dict_list[c3.index(max(c3))]['Mouse-colored Tyrannulet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75e97f",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the preprocessed file path\n",
    "sample_path_0 = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/birds/scannet_3d/example/scene0000_00_vh_clean_2.pth\"\n",
    "#sample_path_1 = \"D:/AT3DCV_Data/Preprocessed_OpenScene/data/scannet_3d/train/scene0000_01_vh_clean_2.pth\"\n",
    "#sample_path_2 = \"D:/AT3DCV_Data/Preprocessed_OpenScene/data/scannet_3d/train/scene0000_02_vh_clean_2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_0 = torch.load(sample_path_0) # coords,colors,labels\n",
    "#sample_1 = torch.load(sample_path_1) # coords,colors,labels\n",
    "#sample_2 = torch.load(sample_path_2) # coords,colors,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d69828",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d643bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating all of the partial point clouds of the same scene (they don't overlap perfectly)\n",
    "#sample_points = np.concatenate((sample_0[0], sample_1[0], sample_2[0]))\n",
    "#sample_colors = np.concatenate((sample_0[1], sample_1[1], sample_2[1]))\n",
    "\n",
    "# single partial point cloud\n",
    "sample_points  = sample_0[0]\n",
    "sample_colors = sample_0[1]\n",
    "sample_labels = sample_0[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6663b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to view original scene\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(sample_points))\n",
    "#original colors\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(sample_colors))\n",
    "#------\n",
    "#paint uniform\n",
    "#sample_paint_uniform = np.asarray([200,200,200])/255.0 #redish\n",
    "#pcd.paint_uniform_color(sample_paint_uniform)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5135af0",
   "metadata": {},
   "source": [
    "# load fused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212cf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the fused feature path\n",
    "feature_path = \"/mnt/project/AT3DCV_Data/Preprocessed_OpenScene/data/augmented/birds/fused/scene0000_00_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = torch.load(feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature[\"mask_full\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f474ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature[\"feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices where the mask is True\n",
    "indices = torch.nonzero(feature[\"mask_full\"]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65077247",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_point_cloud = sample_points[indices, :]\n",
    "filtered_point_cloud_colors = sample_colors[indices, :]\n",
    "filtered_point_cloud_labels = sample_labels[indices]\n",
    "gt_ids = filtered_point_cloud_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(filtered_point_cloud_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafeeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace every occurrence of 21 with 20 if necessary\n",
    "gt_ids= np.where(filtered_point_cloud_labels == 21.0, 20.0, filtered_point_cloud_labels)\n",
    "gt_ids= np.where(gt_ids == 22.0, 20.0, gt_ids)\n",
    "# gt_ids = filtered_point_cloud_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(gt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d482d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(gt_ids, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb65e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_point_cloud.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd5dbb",
   "metadata": {},
   "source": [
    "# using clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight with a threshold\n",
    "# type the query here \n",
    "query = [\"dragon\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_text_embeddings = []\n",
    "    for category in tqdm(query):\n",
    "        texts = clip.tokenize(category)  #tokenize\n",
    "        texts = texts.cuda()\n",
    "        text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embeddings.mean(dim=0)\n",
    "        text_embedding /= text_embedding.norm()\n",
    "        all_text_embeddings.append(text_embedding)\n",
    "\n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "\n",
    "# normalizing \n",
    "fused_f = (feature[\"feat\"]/(feature[\"feat\"].norm(dim=-1, keepdim=True)+1e-5)).half()\n",
    "# calculating similarity matrix\n",
    "# similarity_matrix = torch.matmul(feature[\"feat\"].cuda(), all_text_embeddings) # \n",
    "similarity_matrix = fused_f.cuda() @ all_text_embeddings    \n",
    "    \n",
    "# set higher to increase the certainty (not always correct)\n",
    "threshold_percentage = 0.9\n",
    "cap = similarity_matrix.max().item()\n",
    "found_indices = torch.nonzero(similarity_matrix > cap*threshold_percentage, as_tuple=False).squeeze().T[0]\n",
    "\n",
    "# creating pc\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud))\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud_colors))\n",
    "\n",
    "found_region = pcd.select_by_index(found_indices.tolist())\n",
    "found_region.paint_uniform_color([1.0, 0, 0]) # paint related points to red\n",
    "rest = pcd.select_by_index(found_indices.tolist(), invert=True)\n",
    "o3d.visualization.draw_geometries([rest,found_region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3778aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight with a heatmap\n",
    "# type the query here \n",
    "# query = [\"deathwing\"]\n",
    "# query = [\" a blue-faced, yellow-crowned, white-breasted, black-eyed, long-billed, hooked-beak, yellow-beaked, yellow-breasted, yellow-throated and black-tailed bird\"]\n",
    "\n",
    "# mouse-colored tyrannulet \n",
    "query = [[\"grey-bodied\",\"yellow-breasted\",\"black-crowned\",\n",
    "          \"white-eyed\",\"black-winged\",\"yellow-throated\",\n",
    "          \"white-breasted\",\"yellow-billed\",\"grey-headed\",\n",
    "          \"long-tailed\",\"bird\"]]\n",
    "\n",
    "# diamong firetail\n",
    "query = [[\"red-breasted\",\"black-crowned\",\"gold-winged\",\n",
    "          \"black-winged\",\"white-eyed\",\"yellow-billed\",\n",
    "          \"red-headed\",\"black-tailed\",\"long-tailed\",\n",
    "          \"white-breasted\",\"bird\"]]\n",
    "\n",
    "\n",
    "\n",
    "#query = [\"bird\"]\n",
    "#query = [[\"Mouse-colored Tyrannulet bird\"]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_text_embeddings = []\n",
    "    for category in tqdm(query):\n",
    "        texts = clip.tokenize(category)  #tokenize\n",
    "        texts = texts.cuda()\n",
    "        text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embeddings.mean(dim=0)\n",
    "        text_embedding /= text_embedding.norm()\n",
    "        all_text_embeddings.append(text_embedding)\n",
    "\n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "\n",
    "# normalizing \n",
    "fused_f = (feature[\"feat\"]/(feature[\"feat\"].norm(dim=-1, keepdim=True)+1e-5)).half()\n",
    "# calculating similarity matrix\n",
    "# similarity_matrix = torch.matmul(feature[\"feat\"].cuda(), all_text_embeddings) # \n",
    "similarity_matrix = fused_f.cuda() @ all_text_embeddings    \n",
    "\n",
    "# creating pc\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud))\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.asarray(filtered_point_cloud_colors))\n",
    "\n",
    "# heatmap\n",
    "cmap = plt.get_cmap('cividis')\n",
    "\n",
    "# normalize the tensor to the range [0, 1]\n",
    "normalized_tensor = (similarity_matrix - torch.min(similarity_matrix)) / (torch.max(similarity_matrix) - torch.min(similarity_matrix))\n",
    "\n",
    "colors = cmap(normalized_tensor.detach().cpu().numpy().squeeze())\n",
    "pcd_heatmap = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd_heatmap.points = o3d.utility.Vector3dVector(pcd.points)\n",
    "pcd_heatmap.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "\n",
    "#transform heatmap to the side\n",
    "pcd_heatmap.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) + [0,10,0])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd, pcd_heatmap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4a00d",
   "metadata": {},
   "source": [
    "# mIoU evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANNET_LABELS_20 = ['wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa',\n",
    "                     'table', 'door', 'window', 'bookshelf', 'picture','counter', 'desk', 'curtain', 'refrigerator', 'shower curtain',\n",
    "                     'toilet', 'sink', 'bathtub', 'otherfurniture']\n",
    "UNKNOWN_ID = 255\n",
    "NO_FEATURE_ID = 256\n",
    "\n",
    "SCANNET_LABELS_20.append(query[0])\n",
    "#SCANNET_LABELS_20.append(\"bird\")\n",
    "\n",
    "CLASS_LABELS = SCANNET_LABELS_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    label_embeds = []\n",
    "    for category in tqdm(SCANNET_LABELS_20):\n",
    "        texts = clip.tokenize(category)  #tokenize\n",
    "        texts = texts.cuda()\n",
    "        text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embeddings.mean(dim=0)\n",
    "        text_embedding /= text_embedding.norm()\n",
    "        label_embeds.append(text_embedding)\n",
    "\n",
    "    label_embeds = torch.stack(label_embeds, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('classes          IoU')\n",
    "print('----------------------------')\n",
    "for i in range(N_CLASSES):\n",
    "    label_name = CLASS_LABELS[i]\n",
    "    if not isinstance(label_name, str): label_name = target_label\n",
    "    try:\n",
    "        print('{0:<14s}: {1:>5.5f}   ({2:>6d}/{3:<6d})'.format(\n",
    "                label_name,\n",
    "                class_ious[label_name][0],\n",
    "                class_ious[label_name][1],\n",
    "                class_ious[label_name][2]))\n",
    "    except:\n",
    "        print(label_name + ' error!')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  engine=\"text-davinci-003\",\n",
    "  prompt=\"Could you generate 5 visual descriptors for each of the following object classes, they are bird species: [Blue-faced Honeyeate, Diamond Firetail, Mouse-colored Tyrannulet]. The descriptors will be used for input queries for a CLIP model. The descriptors should be concise and distinct from one another. Do not focus on behavior, but purely on attributes which are recognizable by the CLIP model. The output should be in the following form, without any additional text: object class 1, visual descriptor 1.1, visual descriptor 1.2\",\n",
    "\n",
    "  temperature=0.5,\n",
    "  max_tokens=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version of the aggregating text embeddings, it's not properly working\n",
    "def highlight_query(query, feature_type, model, distill, fused, fpc, fpcc, device):\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_text_embeddings = []\n",
    "        for category in tqdm(query):\n",
    "            texts = clip.tokenize(category)  #tokenize\n",
    "            texts = texts.to(device)\n",
    "            text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_embedding = text_embeddings.mean(dim=0)\n",
    "            text_embedding /= text_embedding.norm()\n",
    "            all_text_embeddings.append(text_embedding)\n",
    "\n",
    "        all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "\n",
    "        \n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ all_text_embeddings\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ all_text_embeddings\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ all_text_embeddings\n",
    "        pred_distill = distill.to(device) @ all_text_embeddings\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble @ all_text_embeddings\n",
    "        \n",
    "    print(similarity_matrix.shape)\n",
    "    # creating pc\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.asarray(fpc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.asarray(fpcc))\n",
    "\n",
    "    # heatmap\n",
    "    cmap = plt.get_cmap('cividis')\n",
    "\n",
    "    # normalize the tensor to the range [0, 1]\n",
    "    normalized_tensor = (similarity_matrix - torch.min(similarity_matrix)) / (torch.max(similarity_matrix) - torch.min(similarity_matrix))\n",
    "\n",
    "    colors = cmap(normalized_tensor.detach().cpu().numpy().squeeze())\n",
    "    pcd_heatmap = o3d.geometry.PointCloud()\n",
    "\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(pcd.points)\n",
    "    pcd_heatmap.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "\n",
    "    #transform heatmap to the side\n",
    "    pcd_heatmap.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) + [0,10,0])\n",
    "\n",
    "    o3d.visualization.draw_geometries([pcd, pcd_heatmap])\n",
    "    \n",
    "def evaluate(labelset, descriptors, feature_type, model, distill, fused, gt_ids):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        label_embeds = []\n",
    "        for category in tqdm(labelset):\n",
    "            texts = clip.tokenize(category)  #tokenize\n",
    "            texts = texts.cuda()\n",
    "            text_embeddings = model.encode_text(texts)  #embed with text encoder\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_embedding = text_embeddings.mean(dim=0)\n",
    "            text_embedding /= text_embedding.norm()\n",
    "            label_embeds.append(text_embedding)\n",
    "\n",
    "        label_embeds = torch.stack(label_embeds, dim=1)\n",
    "        \n",
    "    if feature_type == \"fused\":\n",
    "        similarity_matrix = fused.to(device) @ label_embeds\n",
    "    elif feature_type == \"distilled\":\n",
    "        similarity_matrix = distill.to(device) @ label_embeds\n",
    "    elif feature_type == \"ensembled\":\n",
    "        pred_fusion = fused.to(device) @ label_embeds\n",
    "        pred_distill = distill.to(device) @ label_embeds\n",
    "        feat_ensemble = distill.clone().half()\n",
    "        mask_ = pred_distill.max(dim=-1)[0] < pred_fusion.max(dim=-1)[0]\n",
    "        feat_ensemble[mask_] = fused_f[mask_]\n",
    "        similarity_matrix = feat_ensemble.to(device) @ label_embeds\n",
    "        \n",
    "    pred_ids = torch.max(similarity_matrix, 1)[1].detach().cpu()    \n",
    "    \n",
    "    N_CLASSES = len(labelset)\n",
    "    confusion = confusion_matrix(pred_ids, gt_ids, N_CLASSES)\n",
    "    class_ious = {}\n",
    "    class_accs = {}\n",
    "    mean_iou = 0\n",
    "    mean_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(N_CLASSES):\n",
    "        label_name = labelset[i]\n",
    "\n",
    "        if not isinstance(label_name, str): \n",
    "            for key, value in descriptors.items():\n",
    "                if value == label_name:\n",
    "                    label_name = key\n",
    "                    \n",
    "        if (gt_ids==i).sum() == 0: # at least 1 point needs to be in the evaluation for this class\n",
    "            continue\n",
    "\n",
    "\n",
    "        class_ious[label_name] = get_iou(i, confusion)\n",
    "        class_accs[label_name] = class_ious[label_name][1] / (gt_ids==i).sum()\n",
    "        count+=1\n",
    "\n",
    "        mean_iou += class_ious[label_name][0]\n",
    "        mean_acc += class_accs[label_name]\n",
    "\n",
    "\n",
    "    mean_iou /= N_CLASSES\n",
    "    mean_acc /= N_CLASSES\n",
    "    \n",
    "    return class_ious, class_accs, mean_iou, mean_acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
